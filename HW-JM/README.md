# 反向傳遞演算法（Backpropagation）之梯度引擎實作

---

## 1. 題目

反向傳遞演算法（Reverse-Mode Automatic Differentiation）  
梯度引擎之設計與實作

---

## 2. 概述

反向傳遞演算法（Backpropagation）是現代機器學習與深度學習中最核心的基礎技術之一，幾乎所有以梯度為基礎的模型訓練流程，皆仰賴此機制來計算參數對損失函數的偏微分，並進行有效的參數更新。透過鏈式法則（Chain Rule），反向傳遞能夠在複雜的計算圖中，將誤差由輸出端逐層回傳至各個中間節點與參數。

在實際應用中，多數學習者僅使用現成的深度學習框架進行模型訓練，卻難以清楚理解其背後的自動微分與梯度計算流程。因此，本專題的核心目標在於從底層實作反向模式自動微分（Reverse-Mode Automatic Differentiation）的梯度引擎，以加深對反向傳遞演算法運作細節的理解。

本專題以 Python 為開發語言，完全不依賴任何深度學習框架，透過自行建構計算圖（Computation Graph）、定義各運算的局部梯度規則，並實作反向傳遞流程，完整呈現梯度如何在圖中傳播與累積。同時，透過線性回歸實驗與數值梯度檢查，驗證所實作之梯度引擎在理論與實務上的正確性。

---

## 3. 特徵

- 從零實作反向傳遞演算法，不使用 PyTorch / TensorFlow
- 以計算圖（Computation Graph）為核心設計
- 支援多種基本運算與非線性函數
- 具備自動微分（Automatic Differentiation）能力
- 提供數值梯度檢查（Gradient Check）驗證正確性
- 可輸出計算圖結構，利於學習與除錯

---

## 4. 成分（系統組成）

本系統主要由以下模組構成：

1. **Value 類別**
   - 表示計算圖中的一個純量節點
2. **運算子重載**
   - `+`, `-`, `*`, `/`, `**`
3. **非線性函數**
   - ReLU
   - Tanh
   - Sigmoid（含數值穩定處理）
4. **反向傳遞機制**
   - 拓樸排序
   - 梯度回傳與累積
5. **輔助工具**
   - 計算圖追蹤與文字化輸出
   - 數值梯度檢查工具

---

## 5. 工作原理

1. 每一個數值運算都會產生一個 `Value` 節點
2. 節點之間以父子關係形成有向無環圖（DAG）
3. 前向傳遞時計算數值（data）
4. 同時為每個節點定義對應的局部反向梯度函式
5. 反向傳遞時，從輸出節點開始套用鏈式法則
6. 梯度沿計算圖逐層回傳並累積至所有相關節點

---

## 6. 範例輸出（節錄）

### Demo 1：線性回歸

```text
epoch=  1 loss=38.032337  a=2.1939 b=0.2261
epoch= 20 loss=0.048328  a=2.5032 b=-0.8430
epoch= 40 loss=0.014414  a=2.5032 b=-0.9897
epoch= 60 loss=0.013913  a=2.5032 b=-1.0076
epoch= 80 loss=0.013905  a=2.5032 b=-1.0097
epoch=100 loss=0.013905  a=2.5032 b=-1.0100
epoch=120 loss=0.013905  a=2.5032 b=-1.0100
epoch=140 loss=0.013905  a=2.5032 b=-1.0100
epoch=160 loss=0.013905  a=2.5032 b=-1.0100
epoch=180 loss=0.013905  a=2.5032 b=-1.0100
epoch=200 loss=0.013905  a=2.5032 b=-1.0100

True params: 2.5 -1.0
Learned params: 2.5031983333845145 -1.0100261165178162
```

### Demo 2：梯度檢查

```text
x.data = 1.234500
f(x)   = 1.921514
autodiff grad df/dx = 0.1485876193
numerical grad      = 0.1485876193
abs diff            = 6.5606492461e-11
```
